{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df054e37-72f7-4ecf-b459-5a89d382a346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Validation Notebook\n",
    "\n",
    "This notebook performs model validation using the MLflow model validation API after training and registering a model in the model registry, prior to deployment to the \"champion\" alias. It is designed to run as part of a continuous deployment (CD) pipeline, triggered by an automated model training job, followed by validation and deployment, as defined in `mlops_dbx/resources/model-workflow-resource.yml`.\n",
    "\n",
    "Parameters:\n",
    "    - experiment_name: Name of the MLflow experiment.\n",
    "    - run_mode: Mode for model validation. Options:\n",
    "        - disabled: Skip validation and allow deployment.\n",
    "        - dry_run: Run validation, ignore failures, and allow deployment.\n",
    "        - enabled: Run validation, block deployment if validation fails.\n",
    "    - enable_baseline_comparison: Whether to load the current \"champion\" model as baseline for comparison.\n",
    "    - validation_input: Input table for validation data.\n",
    "    - model_type: Type of model (\"regressor\" or \"classifier\").\n",
    "    - targets: Name of the column containing evaluation labels.\n",
    "    - custom_metrics_loader_function: Function name to load custom metrics.\n",
    "    - validation_thresholds_loader_function: Function name to load validation thresholds.\n",
    "    - evaluator_config_loader_function: Function name to load evaluator config.\n",
    "    - model_name: Full model name in registry.\n",
    "    - model_version: Candidate model alias/version.\n",
    "\n",
    "Workflow:\n",
    "    1. Reads parameters from widgets.\n",
    "    2. Sets up MLflow experiment and model URIs.\n",
    "    3. Loads validation data and configuration.\n",
    "    4. Loads custom metrics, thresholds, and evaluator config.\n",
    "    5. Runs model evaluation using MLflow, optionally compares with baseline.\n",
    "    6. Logs validation results and metrics as artifacts.\n",
    "    7. Validates evaluation results against thresholds.\n",
    "    8. Updates model version description and assigns \"challenger\" alias if validation passes.\n",
    "    9. Handles validation failures according to run_mode.\n",
    "\n",
    "Notes:\n",
    "    - Baseline comparison is currently disabled for models registered with Feature Store.\n",
    "    - Uses Databricks Feature Store for batch scoring.\n",
    "    - Artifacts and metrics are logged to MLflow for traceability.\n",
    "    - Model validation status is appended to model version description in registry.\n",
    "\n",
    "References:\n",
    "    - MLflow evaluate API: https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate\n",
    "    - Model Validation documentation: https://mlflow.org/docs/latest/models.html#model-validation\n",
    "    - Feature Store limitation: https://github.com/databricks/mlops-stacks/issues/70\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4775f5-a1da-445d-931b-c0b820318feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\n",
    "    \"experiment_name\",\n",
    "    \"/Workspace/Shared/mlops_talk/telco_churn_model\",\n",
    "    \"Experiment Name\",\n",
    ")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"enabled\", [\"disabled\", \"dry_run\", \"enabled\"], \"Run Mode\")\n",
    "dbutils.widgets.dropdown(\"enable_baseline_comparison\", \"false\", [\"true\", \"false\"], \"Enable Baseline Comparison\")\n",
    "dbutils.widgets.text(\"validation_input\", \"mlops_dbx_talk_dev.churn.telco_churn_validation\", \"Validation Input\")\n",
    "\n",
    "dbutils.widgets.text(\"model_type\", \"classifier\", \"Model Type\")\n",
    "dbutils.widgets.text(\"targets\", \"churn\", \"Targets\")\n",
    "dbutils.widgets.text(\"custom_metrics_loader_function\", \"custom_metrics\", \"Custom Metrics Loader Function\")\n",
    "dbutils.widgets.text(\"validation_thresholds_loader_function\", \"validation_thresholds\", \"Validation Thresholds Loader Function\")\n",
    "dbutils.widgets.text(\"evaluator_config_loader_function\", \"evaluator_config\", \"Evaluator Config Loader Function\")\n",
    "dbutils.widgets.text(\"model_name\", \"mlops_dbx_talk_dev.churn.telco_churn_model\", \"Full (Three-Level) Model Name\")\n",
    "dbutils.widgets.text(\"model_version\", \"staging\", \"Candidate Model Alias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859dff44-0c94-459e-9588-aae6579d3cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_mode = dbutils.widgets.get(\"run_mode\").lower()\n",
    "assert run_mode == \"disabled\" or run_mode == \"dry_run\" or run_mode == \"enabled\"\n",
    "\n",
    "if run_mode == \"disabled\":\n",
    "    print(\n",
    "        \"Model validation is in DISABLED mode. Exit model validation without blocking model deployment.\"\n",
    "    )\n",
    "    dbutils.notebook.exit(0)\n",
    "dry_run = run_mode == \"dry_run\"\n",
    "\n",
    "if dry_run:\n",
    "    print(\n",
    "        \"Model validation is in DRY_RUN mode. Validation threshold validation failures will not block model deployment.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Model validation is in ENABLED mode. Validation threshold validation failures will block model deployment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27794327-9276-46ba-9758-33c02ca1cc09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mlflow\n",
    "import os\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.models.evaluation.base import EvaluationResult\n",
    "\n",
    "client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# set experiment\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# set model evaluation parameters that can be inferred from the job\n",
    "# model_uri = dbutils.jobs.taskValues.get(\"Train\", \"model_uri\", debugValue=\"\")\n",
    "# model_name = dbutils.jobs.taskValues.get(\"Train\", \"model_name\", debugValue=\"\")\n",
    "# model_version = dbutils.jobs.taskValues.get(\"Train\", \"model_version\", debugValue=\"\")\n",
    "\n",
    "# if model_uri == \"\":\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "model_version = dbutils.widgets.get(\"model_version\")\n",
    "model_uri = \"models:/\" + model_name + \"@\" + model_version\n",
    "\n",
    "baseline_model_uri = \"models:/\" + model_name + \"@champion\"\n",
    "\n",
    "evaluators = \"default\"\n",
    "assert model_uri != \"\", \"model_uri notebook parameter must be specified\"\n",
    "assert model_name != \"\", \"model_name notebook parameter must be specified\"\n",
    "assert model_version != \"\", \"model_version notebook parameter must be specified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940b51fd-50f1-446d-9791-2cde9dd7e2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take input\n",
    "enable_baseline_comparison = dbutils.widgets.get(\"enable_baseline_comparison\")\n",
    "\n",
    "\n",
    "enable_baseline_comparison = \"false\" \n",
    "print(\n",
    "    \"Currently baseline model comparison is not supported for models registered with feature store. Please refer to \"\n",
    "    \"issue https://github.com/databricks/mlops-stacks/issues/70 for more details.\"\n",
    ")\n",
    "\n",
    "assert enable_baseline_comparison == \"true\" or enable_baseline_comparison == \"false\"\n",
    "enable_baseline_comparison = enable_baseline_comparison == \"true\"\n",
    "\n",
    "validation_input = dbutils.widgets.get(\"validation_input\")\n",
    "assert validation_input\n",
    "data = spark.table(validation_input)\n",
    "\n",
    "model_type = dbutils.widgets.get(\"model_type\")\n",
    "targets = dbutils.widgets.get(\"targets\")\n",
    "\n",
    "assert model_type\n",
    "assert targets\n",
    "\n",
    "custom_metrics_loader_function_name = dbutils.widgets.get(\"custom_metrics_loader_function\")\n",
    "validation_thresholds_loader_function_name = dbutils.widgets.get(\"validation_thresholds_loader_function\")\n",
    "evaluator_config_loader_function_name = dbutils.widgets.get(\"evaluator_config_loader_function\")\n",
    "assert custom_metrics_loader_function_name\n",
    "assert validation_thresholds_loader_function_name\n",
    "assert evaluator_config_loader_function_name\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from validation.validation import custom_metrics, validation_thresholds, evaluator_config\n",
    "\n",
    "validation_thresholds = validation_thresholds()\n",
    "custom_metrics = custom_metrics()\n",
    "evaluator_config = evaluator_config()\n",
    "\n",
    "# custom_metrics_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), custom_metrics_loader_function_name\n",
    "# )\n",
    "# validation_thresholds_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), validation_thresholds_loader_function_name\n",
    "# )\n",
    "# evaluator_config_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), evaluator_config_loader_function_name\n",
    "# )\n",
    "# custom_metrics = custom_metrics_loader_function()\n",
    "# validation_thresholds = validation_thresholds_loader_function()\n",
    "# evaluator_config = evaluator_config_loader_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fda376-2a8c-4706-a55e-0628e4584c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper methods\n",
    "def get_run_link(run_info):\n",
    "    return \"[Run](#mlflow/experiments/{0}/runs/{1})\".format(\n",
    "        run_info.experiment_id, run_info.run_id\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_run(model_name, model_version):\n",
    "    version = client.get_model_version_by_alias(model_name, model_version)\n",
    "    return mlflow.get_run(run_id=version.run_id)\n",
    "\n",
    "\n",
    "def generate_run_name(training_run):\n",
    "    return None if not training_run else training_run.info.run_name + \"-validation\"\n",
    "\n",
    "\n",
    "def generate_description(training_run):\n",
    "    return (\n",
    "        None\n",
    "        if not training_run\n",
    "        else \"Model Training Details: {0}\\n\".format(get_run_link(training_run.info))\n",
    "    )\n",
    "\n",
    "\n",
    "def log_to_model_description(run, success):\n",
    "    run_link = get_run_link(run.info)\n",
    "    description = client.get_model_version_by_alias(model_name, model_version).description\n",
    "    version = client.get_model_version_by_alias(model_name, model_version).version\n",
    "    status = \"SUCCESS\" if success else \"FAILURE\"\n",
    "    if description != \"\":\n",
    "        description += \"\\n\\n---\\n\\n\"\n",
    "    description += \"Model Validation Status: {0}\\nValidation Details: {1}\".format(\n",
    "        status, run_link\n",
    "    )\n",
    "    client.update_model_version(\n",
    "        name=model_name, version=version, description=description\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455d484f-5f42-48bf-b257-e774cd21c358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Temporary fix as FS model can't predict as a pyfunc model\n",
    "# MLflow evaluate can take a lambda function instead of a model uri for a model\n",
    "# but id does not work for the baseline model as it requires a model_uri (baseline comparison is set to false)\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "def get_fs_model(df, model_uri):\n",
    "    fs_client = FeatureStoreClient()\n",
    "    return (\n",
    "        fs_client.score_batch(model_uri, spark.createDataFrame(df))\n",
    "        # .select(\"prediction\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "\n",
    "training_run = get_training_run(model_name, model_version)\n",
    "\n",
    "# run evaluate\n",
    "with mlflow.start_run(\n",
    "    run_name=generate_run_name(training_run),\n",
    "    description=generate_description(training_run),\n",
    ") as run, tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    validation_thresholds_file = os.path.join(tmp_dir, \"validation_thresholds.txt\")\n",
    "    with open(validation_thresholds_file, \"w\") as f:\n",
    "        if validation_thresholds:\n",
    "            for metric_name in validation_thresholds:\n",
    "                f.write(\n",
    "                    \"{0:30}  {1}\\n\".format(\n",
    "                        metric_name, str(validation_thresholds[metric_name])\n",
    "                    )\n",
    "                )\n",
    "    mlflow.log_artifact(validation_thresholds_file)\n",
    "\n",
    "    try:\n",
    "        eval_result = mlflow.evaluate(\n",
    "            # model=get_fs_model,\n",
    "            data=get_fs_model(data.toPandas(), model_uri),\n",
    "            targets=targets,\n",
    "            predictions=\"prediction\",\n",
    "            model_type=model_type,\n",
    "            evaluators=evaluators,\n",
    "            extra_metrics=custom_metrics,\n",
    "            evaluator_config=evaluator_config,\n",
    "        )\n",
    "        if enable_baseline_comparison:\n",
    "            baseline_eval_result = mlflow.evaluate(\n",
    "                # model=get_fs_model,\n",
    "                data=get_fs_model(data.toPandas(), baseline_model_uri),\n",
    "                targets=targets,\n",
    "                predictions=\"prediction\",\n",
    "                model_type=model_type,\n",
    "                evaluators=evaluators,\n",
    "                extra_metrics=custom_metrics,\n",
    "                evaluator_config=evaluator_config,\n",
    "            )\n",
    "        else:\n",
    "            baseline_eval_result = None\n",
    "            \n",
    "        metrics_file = os.path.join(tmp_dir, \"metrics.txt\")\n",
    "        with open(metrics_file, \"w\") as f:\n",
    "            f.write(\n",
    "                \"{0:30}  {1:30}  {2}\\n\".format(\"metric_name\", \"candidate\", \"baseline\")\n",
    "            )\n",
    "            for metric in eval_result.metrics:\n",
    "                candidate_metric_value = str(eval_result.metrics[metric])\n",
    "                baseline_metric_value = \"N/A\"\n",
    "                if (baseline_eval_result is not None) and (metric in baseline_eval_result.metrics):\n",
    "                    mlflow.log_metric(\n",
    "                        \"baseline_\" + metric, eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                    baseline_metric_value = str(\n",
    "                        eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                f.write(\n",
    "                    \"{0:30}  {1:30}  {2}\\n\".format(\n",
    "                        metric, candidate_metric_value, baseline_metric_value\n",
    "                    )\n",
    "                )\n",
    "        mlflow.log_artifact(metrics_file)\n",
    "        \n",
    "        mlflow.validate_evaluation_results(validation_thresholds, eval_result, baseline_eval_result)\n",
    "\n",
    "        log_to_model_description(run, True)\n",
    "        version = client.get_model_version_by_alias(model_name, model_version).version\n",
    "        \n",
    "        # Assign \"challenger\" alias to indicate model version has passed validation checks\n",
    "        print(\"Validation checks passed. Assigning 'challenger' alias to model version.\")\n",
    "        client.set_registered_model_alias(model_name, \"challenger\", version)\n",
    "        client.delete_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"staging\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        raise ValueError(err)\n",
    "        # log_to_model_description(run, False)\n",
    "        # error_file = os.path.join(tmp_dir, \"error.txt\")\n",
    "        # with open(error_file, \"w\") as f:\n",
    "        #     f.write(\"Validation failed : \" + str(err) + \"\\n\")\n",
    "        #     f.write(traceback.format_exc())\n",
    "        # mlflow.log_artifact(error_file)\n",
    "        # if not dry_run:\n",
    "        #     raise err\n",
    "        # else:\n",
    "        #     print(\n",
    "        #         \"Model validation failed in DRY_RUN. It will not block model deployment.\"\n",
    "        #     )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "databricks-feature-engineering"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ModelValidation",
   "widgets": {
    "custom_metrics_loader_function": {
     "currentValue": "custom_metrics",
     "nuid": "740351f0-4734-490f-983e-76797bc28060",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "custom_metrics",
      "label": "Custom Metrics Loader Function",
      "name": "custom_metrics_loader_function",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "custom_metrics",
      "label": "Custom Metrics Loader Function",
      "name": "custom_metrics_loader_function",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "enable_baseline_comparison": {
     "currentValue": "true",
     "nuid": "58b94d93-cfbe-486a-a03e-7db9fbda79e8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Enable Baseline Comparison",
      "name": "enable_baseline_comparison",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Enable Baseline Comparison",
      "name": "enable_baseline_comparison",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "evaluator_config_loader_function": {
     "currentValue": "evaluator_config",
     "nuid": "b75de9aa-02ef-4ff9-8ac4-3656becf4e23",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "evaluator_config",
      "label": "Evaluator Config Loader Function",
      "name": "evaluator_config_loader_function",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "evaluator_config",
      "label": "Evaluator Config Loader Function",
      "name": "evaluator_config_loader_function",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "experiment_name": {
     "currentValue": "/Workspace/Shared/mlops_talk/telco_churn_model",
     "nuid": "25ccb47f-bf97-49bb-8909-c216afb589d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Shared/mlops_talk/telco_churn_model",
      "label": "Experiment Name",
      "name": "experiment_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Shared/mlops_talk/telco_churn_model",
      "label": "Experiment Name",
      "name": "experiment_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_name": {
     "currentValue": "mlops_dbx_talk_dev.churn.telco_churn_model",
     "nuid": "7e37dac5-0a3e-4049-bb83-d7ca6af3f0db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mlops_dbx_talk_dev.churn.telco_churn_model",
      "label": "Full (Three-Level) Model Name",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mlops_dbx_talk_dev.churn.telco_churn_model",
      "label": "Full (Three-Level) Model Name",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_type": {
     "currentValue": "classifier",
     "nuid": "d5b7ca07-4533-4f8e-951f-67b2705389b1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "classifier",
      "label": "Model Type",
      "name": "model_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "classifier",
      "label": "Model Type",
      "name": "model_type",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_version": {
     "currentValue": "staging",
     "nuid": "7ec3bafe-e0cf-4766-8593-dbbad55428d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "staging",
      "label": "Candidate Model Alias",
      "name": "model_version",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "staging",
      "label": "Candidate Model Alias",
      "name": "model_version",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "run_mode": {
     "currentValue": "enabled",
     "nuid": "8e359548-9387-4923-aeab-7d421e160eef",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "enabled",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "disabled",
        "dry_run",
        "enabled"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "enabled",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "disabled",
        "dry_run",
        "enabled"
       ]
      }
     }
    },
    "targets": {
     "currentValue": "churn",
     "nuid": "22176e07-1da5-45a5-99e5-95f2a0b6949c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "churn",
      "label": "Targets",
      "name": "targets",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "churn",
      "label": "Targets",
      "name": "targets",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "validation_input": {
     "currentValue": "mlops_dbx_talk_dev.churn.telco_churn_validation",
     "nuid": "4f184c4a-df6b-46ef-8106-298105f70f20",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mlops_dbx_talk_dev.churn.telco_churn_validation",
      "label": "Validation Input",
      "name": "validation_input",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mlops_dbx_talk_dev.churn.telco_churn_validation",
      "label": "Validation Input",
      "name": "validation_input",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "validation_thresholds_loader_function": {
     "currentValue": "validation_thresholds",
     "nuid": "d34d43f5-a41b-4277-aa62-09c4b974c9e8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "validation_thresholds",
      "label": "Validation Thresholds Loader Function",
      "name": "validation_thresholds_loader_function",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "validation_thresholds",
      "label": "Validation Thresholds Loader Function",
      "name": "validation_thresholds_loader_function",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
