{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c2dbd8-29a0-4809-a8b9-c38f47ee2ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Model Validation Notebook\n",
    "##\n",
    "# This notebook uses mlflow model validation API to run mode validation after training and registering a model\n",
    "# in model registry, before deploying it to the \"champion\" alias.\n",
    "#\n",
    "# It runs as part of CD and by an automated model training job -> validation -> deployment job defined under ``mlops_dbx/resources/model-workflow-resource.yml``\n",
    "#\n",
    "#\n",
    "# Parameters:\n",
    "#\n",
    "# * env                                     - Name of the environment the notebook is run in (staging, or prod). Defaults to \"prod\".\n",
    "# * `run_mode`                              - The `run_mode` defines whether model validation is enabled or not. It can be one of the three values:\n",
    "#                                             * `disabled` : Do not run the model validation notebook.\n",
    "#                                             * `dry_run`  : Run the model validation notebook. Ignore failed model validation rules and proceed to move\n",
    "#                                                            model to the \"champion\" alias.\n",
    "#                                             * `enabled`  : Run the model validation notebook. Move model to the \"champion\" alias only if all model validation\n",
    "#                                                            rules are passing.\n",
    "# * enable_baseline_comparison              - Whether to load the current registered \"champion\" model as baseline.\n",
    "#                                             Baseline model is a requirement for relative change and absolute change validation thresholds.\n",
    "# * validation_input                        - Validation input. Please refer to data parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate\n",
    "# * model_type                              - A string describing the model type. The model type can be either \"regressor\" and \"classifier\".\n",
    "#                                             Please refer to model_type parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate\n",
    "# * targets                                 - The string name of a column from data that contains evaluation labels.\n",
    "#                                             Please refer to targets parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate\n",
    "# * custom_metrics_loader_function          - Specifies the name of the function in mlops_dbx/validation/validation.py that returns custom metrics.\n",
    "# * validation_thresholds_loader_function   - Specifies the name of the function in mlops_dbx/validation/validation.py that returns model validation thresholds.\n",
    "#\n",
    "# For details on mlflow evaluate API, see doc https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate\n",
    "# For details and examples about performing model validation, see the Model Validation documentation https://mlflow.org/docs/latest/models.html#model-validation\n",
    "#\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32080432-6556-40f8-aa56-647c66cf4fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d067f4df-02e5-44cc-849a-dc7507fbf90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "%cd $notebook_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0986f46f-a233-4636-8f9e-6f9a68699bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b948ba3-5ee0-4d2d-ac62-7eeb028606d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a56b4a-2a4b-4fb1-a3d7-39921ac63e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "# %cd $notebook_path\n",
    "# %cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4775f5-a1da-445d-931b-c0b820318feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\n",
    "#     \"experiment_name\",\n",
    "#     \"/dev-mlops_dbx-experiment\",\n",
    "#     \"Experiment Name\",\n",
    "# )\n",
    "# dbutils.widgets.dropdown(\"run_mode\", \"disabled\", [\"disabled\", \"dry_run\", \"enabled\"], \"Run Mode\")\n",
    "# dbutils.widgets.dropdown(\"enable_baseline_comparison\", \"false\", [\"true\", \"false\"], \"Enable Baseline Comparison\")\n",
    "# dbutils.widgets.text(\"validation_input\", \"SELECT * FROM delta.`dbfs:/databricks-datasets/nyctaxi-with-zipcodes/subsampled`\", \"Validation Input\")\n",
    "\n",
    "# dbutils.widgets.text(\"model_type\", \"regressor\", \"Model Type\")\n",
    "# dbutils.widgets.text(\"targets\", \"fare_amount\", \"Targets\")\n",
    "# dbutils.widgets.text(\"custom_metrics_loader_function\", \"custom_metrics\", \"Custom Metrics Loader Function\")\n",
    "# dbutils.widgets.text(\"validation_thresholds_loader_function\", \"validation_thresholds\", \"Validation Thresholds Loader Function\")\n",
    "# dbutils.widgets.text(\"evaluator_config_loader_function\", \"evaluator_config\", \"Evaluator Config Loader Function\")\n",
    "# dbutils.widgets.text(\"model_name\", \"dev.mlops_dbx.mlops_dbx-model\", \"Full (Three-Level) Model Name\")\n",
    "# dbutils.widgets.text(\"model_version\", \"\", \"Candidate Model Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859dff44-0c94-459e-9588-aae6579d3cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_mode = dbutils.widgets.get(\"run_mode\").lower()\n",
    "assert run_mode == \"disabled\" or run_mode == \"dry_run\" or run_mode == \"enabled\"\n",
    "\n",
    "if run_mode == \"disabled\":\n",
    "    print(\n",
    "        \"Model validation is in DISABLED mode. Exit model validation without blocking model deployment.\"\n",
    "    )\n",
    "    dbutils.notebook.exit(0)\n",
    "dry_run = run_mode == \"dry_run\"\n",
    "\n",
    "if dry_run:\n",
    "    print(\n",
    "        \"Model validation is in DRY_RUN mode. Validation threshold validation failures will not block model deployment.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Model validation is in ENABLED mode. Validation threshold validation failures will block model deployment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27794327-9276-46ba-9758-33c02ca1cc09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mlflow\n",
    "import os\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.models.evaluation.base import EvaluationResult\n",
    "\n",
    "client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# set experiment\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# set model evaluation parameters that can be inferred from the job\n",
    "# model_uri = dbutils.jobs.taskValues.get(\"Train\", \"model_uri\", debugValue=\"\")\n",
    "# model_name = dbutils.jobs.taskValues.get(\"Train\", \"model_name\", debugValue=\"\")\n",
    "# model_version = dbutils.jobs.taskValues.get(\"Train\", \"model_version\", debugValue=\"\")\n",
    "\n",
    "# if model_uri == \"\":\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "model_version = dbutils.widgets.get(\"model_version\")\n",
    "model_uri = \"models:/\" + model_name + \"@\" + model_version\n",
    "\n",
    "baseline_model_uri = \"models:/\" + model_name + \"@champion\"\n",
    "\n",
    "evaluators = \"default\"\n",
    "assert model_uri != \"\", \"model_uri notebook parameter must be specified\"\n",
    "assert model_name != \"\", \"model_name notebook parameter must be specified\"\n",
    "assert model_version != \"\", \"model_version notebook parameter must be specified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940b51fd-50f1-446d-9791-2cde9dd7e2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take input\n",
    "enable_baseline_comparison = dbutils.widgets.get(\"enable_baseline_comparison\")\n",
    "\n",
    "\n",
    "enable_baseline_comparison = \"false\" \n",
    "print(\n",
    "    \"Currently baseline model comparison is not supported for models registered with feature store. Please refer to \"\n",
    "    \"issue https://github.com/databricks/mlops-stacks/issues/70 for more details.\"\n",
    ")\n",
    "\n",
    "assert enable_baseline_comparison == \"true\" or enable_baseline_comparison == \"false\"\n",
    "enable_baseline_comparison = enable_baseline_comparison == \"true\"\n",
    "\n",
    "validation_input = dbutils.widgets.get(\"validation_input\")\n",
    "assert validation_input\n",
    "data = spark.table(validation_input)\n",
    "\n",
    "model_type = dbutils.widgets.get(\"model_type\")\n",
    "targets = dbutils.widgets.get(\"targets\")\n",
    "\n",
    "assert model_type\n",
    "assert targets\n",
    "\n",
    "custom_metrics_loader_function_name = dbutils.widgets.get(\"custom_metrics_loader_function\")\n",
    "validation_thresholds_loader_function_name = dbutils.widgets.get(\"validation_thresholds_loader_function\")\n",
    "evaluator_config_loader_function_name = dbutils.widgets.get(\"evaluator_config_loader_function\")\n",
    "assert custom_metrics_loader_function_name\n",
    "assert validation_thresholds_loader_function_name\n",
    "assert evaluator_config_loader_function_name\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from validation.validation import custom_metrics, validation_thresholds, evaluator_config\n",
    "\n",
    "validation_thresholds = validation_thresholds()\n",
    "custom_metrics = custom_metrics()\n",
    "evaluator_config = evaluator_config()\n",
    "\n",
    "# custom_metrics_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), custom_metrics_loader_function_name\n",
    "# )\n",
    "# validation_thresholds_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), validation_thresholds_loader_function_name\n",
    "# )\n",
    "# evaluator_config_loader_function = getattr(\n",
    "#     importlib.import_module(\"validation\"), evaluator_config_loader_function_name\n",
    "# )\n",
    "# custom_metrics = custom_metrics_loader_function()\n",
    "# validation_thresholds = validation_thresholds_loader_function()\n",
    "# evaluator_config = evaluator_config_loader_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fda376-2a8c-4706-a55e-0628e4584c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper methods\n",
    "def get_run_link(run_info):\n",
    "    return \"[Run](#mlflow/experiments/{0}/runs/{1})\".format(\n",
    "        run_info.experiment_id, run_info.run_id\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_run(model_name, model_version):\n",
    "    version = client.get_model_version_by_alias(model_name, model_version)\n",
    "    return mlflow.get_run(run_id=version.run_id)\n",
    "\n",
    "\n",
    "def generate_run_name(training_run):\n",
    "    return None if not training_run else training_run.info.run_name + \"-validation\"\n",
    "\n",
    "\n",
    "def generate_description(training_run):\n",
    "    return (\n",
    "        None\n",
    "        if not training_run\n",
    "        else \"Model Training Details: {0}\\n\".format(get_run_link(training_run.info))\n",
    "    )\n",
    "\n",
    "\n",
    "def log_to_model_description(run, success):\n",
    "    run_link = get_run_link(run.info)\n",
    "    description = client.get_model_version_by_alias(model_name, model_version).description\n",
    "    version = client.get_model_version_by_alias(model_name, model_version).version\n",
    "    status = \"SUCCESS\" if success else \"FAILURE\"\n",
    "    if description != \"\":\n",
    "        description += \"\\n\\n---\\n\\n\"\n",
    "    description += \"Model Validation Status: {0}\\nValidation Details: {1}\".format(\n",
    "        status, run_link\n",
    "    )\n",
    "    client.update_model_version(\n",
    "        name=model_name, version=version, description=description\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455d484f-5f42-48bf-b257-e774cd21c358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Temporary fix as FS model can't predict as a pyfunc model\n",
    "# MLflow evaluate can take a lambda function instead of a model uri for a model\n",
    "# but id does not work for the baseline model as it requires a model_uri (baseline comparison is set to false)\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "def get_fs_model(df, model_uri):\n",
    "    fs_client = FeatureStoreClient()\n",
    "    return (\n",
    "        fs_client.score_batch(model_uri, spark.createDataFrame(df))\n",
    "        # .select(\"prediction\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "\n",
    "training_run = get_training_run(model_name, model_version)\n",
    "\n",
    "# run evaluate\n",
    "with mlflow.start_run(\n",
    "    run_name=generate_run_name(training_run),\n",
    "    description=generate_description(training_run),\n",
    ") as run, tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    validation_thresholds_file = os.path.join(tmp_dir, \"validation_thresholds.txt\")\n",
    "    with open(validation_thresholds_file, \"w\") as f:\n",
    "        if validation_thresholds:\n",
    "            for metric_name in validation_thresholds:\n",
    "                f.write(\n",
    "                    \"{0:30}  {1}\\n\".format(\n",
    "                        metric_name, str(validation_thresholds[metric_name])\n",
    "                    )\n",
    "                )\n",
    "    mlflow.log_artifact(validation_thresholds_file)\n",
    "\n",
    "    try:\n",
    "        eval_result = mlflow.evaluate(\n",
    "            # model=get_fs_model,\n",
    "            data=get_fs_model(data.toPandas(), model_uri),\n",
    "            targets=targets,\n",
    "            predictions=\"prediction\",\n",
    "            model_type=model_type,\n",
    "            evaluators=evaluators,\n",
    "            extra_metrics=custom_metrics,\n",
    "            evaluator_config=evaluator_config,\n",
    "        )\n",
    "        if enable_baseline_comparison:\n",
    "            baseline_eval_result = mlflow.evaluate(\n",
    "                # model=get_fs_model,\n",
    "                data=get_fs_model(data.toPandas(), baseline_model_uri),\n",
    "                targets=targets,\n",
    "                predictions=\"prediction\",\n",
    "                model_type=model_type,\n",
    "                evaluators=evaluators,\n",
    "                extra_metrics=custom_metrics,\n",
    "                evaluator_config=evaluator_config,\n",
    "            )\n",
    "        else:\n",
    "            baseline_eval_result = None\n",
    "            \n",
    "        metrics_file = os.path.join(tmp_dir, \"metrics.txt\")\n",
    "        with open(metrics_file, \"w\") as f:\n",
    "            f.write(\n",
    "                \"{0:30}  {1:30}  {2}\\n\".format(\"metric_name\", \"candidate\", \"baseline\")\n",
    "            )\n",
    "            for metric in eval_result.metrics:\n",
    "                candidate_metric_value = str(eval_result.metrics[metric])\n",
    "                baseline_metric_value = \"N/A\"\n",
    "                if (baseline_eval_result is not None) and (metric in baseline_eval_result.metrics):\n",
    "                    mlflow.log_metric(\n",
    "                        \"baseline_\" + metric, eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                    baseline_metric_value = str(\n",
    "                        eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                f.write(\n",
    "                    \"{0:30}  {1:30}  {2}\\n\".format(\n",
    "                        metric, candidate_metric_value, baseline_metric_value\n",
    "                    )\n",
    "                )\n",
    "        mlflow.log_artifact(metrics_file)\n",
    "        \n",
    "        mlflow.validate_evaluation_results(validation_thresholds, eval_result, baseline_eval_result)\n",
    "\n",
    "        log_to_model_description(run, True)\n",
    "        version = client.get_model_version_by_alias(model_name, model_version).version\n",
    "        \n",
    "        # Assign \"challenger\" alias to indicate model version has passed validation checks\n",
    "        print(\"Validation checks passed. Assigning 'challenger' alias to model version.\")\n",
    "        client.set_registered_model_alias(model_name, \"challenger\", version)\n",
    "        client.delete_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"staging\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        raise ValueError(err)\n",
    "        # log_to_model_description(run, False)\n",
    "        # error_file = os.path.join(tmp_dir, \"error.txt\")\n",
    "        # with open(error_file, \"w\") as f:\n",
    "        #     f.write(\"Validation failed : \" + str(err) + \"\\n\")\n",
    "        #     f.write(traceback.format_exc())\n",
    "        # mlflow.log_artifact(error_file)\n",
    "        # if not dry_run:\n",
    "        #     raise err\n",
    "        # else:\n",
    "        #     print(\n",
    "        #         \"Model validation failed in DRY_RUN. It will not block model deployment.\"\n",
    "        #     )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "databricks-feature-engineering"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ModelValidation",
   "widgets": {
    "custom_metrics_loader_function": {
     "currentValue": "custom_metrics",
     "nuid": "740351f0-4734-490f-983e-76797bc28060",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "custom_metrics",
      "label": "Custom Metrics Loader Function",
      "name": "custom_metrics_loader_function",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "custom_metrics",
      "label": "Custom Metrics Loader Function",
      "name": "custom_metrics_loader_function",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "enable_baseline_comparison": {
     "currentValue": "false",
     "nuid": "58b94d93-cfbe-486a-a03e-7db9fbda79e8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Enable Baseline Comparison",
      "name": "enable_baseline_comparison",
      "options": {
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": "Enable Baseline Comparison",
      "name": "enable_baseline_comparison",
      "options": {
       "autoCreated": false,
       "choices": [
        "true",
        "false"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "evaluator_config_loader_function": {
     "currentValue": "evaluator_config",
     "nuid": "b75de9aa-02ef-4ff9-8ac4-3656becf4e23",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "evaluator_config",
      "label": "Evaluator Config Loader Function",
      "name": "evaluator_config_loader_function",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "evaluator_config",
      "label": "Evaluator Config Loader Function",
      "name": "evaluator_config_loader_function",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "experiment_name": {
     "currentValue": "/Workspace/Shared/mlops_talk/telco_churn_model",
     "nuid": "25ccb47f-bf97-49bb-8909-c216afb589d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/dev-mlops_dbx-experiment",
      "label": "Experiment Name",
      "name": "experiment_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/dev-mlops_dbx-experiment",
      "label": "Experiment Name",
      "name": "experiment_name",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_name": {
     "currentValue": "mlops_dbx_talk_dev.churn.telco_churn_model",
     "nuid": "7e37dac5-0a3e-4049-bb83-d7ca6af3f0db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev.mlops_dbx.mlops_dbx-model",
      "label": "Full (Three-Level) Model Name",
      "name": "model_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "dev.mlops_dbx.mlops_dbx-model",
      "label": "Full (Three-Level) Model Name",
      "name": "model_name",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_type": {
     "currentValue": "classifier",
     "nuid": "d5b7ca07-4533-4f8e-951f-67b2705389b1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "regressor",
      "label": "Model Type",
      "name": "model_type",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "regressor",
      "label": "Model Type",
      "name": "model_type",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_version": {
     "currentValue": "staging",
     "nuid": "7ec3bafe-e0cf-4766-8593-dbbad55428d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Candidate Model Alias",
      "name": "model_version",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Candidate Model Alias",
      "name": "model_version",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "run_mode": {
     "currentValue": "enabled",
     "nuid": "8e359548-9387-4923-aeab-7d421e160eef",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "disabled",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "choices": [
        "disabled",
        "dry_run",
        "enabled"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "disabled",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "autoCreated": false,
       "choices": [
        "disabled",
        "dry_run",
        "enabled"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "targets": {
     "currentValue": "churn",
     "nuid": "22176e07-1da5-45a5-99e5-95f2a0b6949c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "fare_amount",
      "label": "Targets",
      "name": "targets",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "fare_amount",
      "label": "Targets",
      "name": "targets",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "validation_input": {
     "currentValue": "mlops_dbx_talk_dev.churn.telco_churn_validation",
     "nuid": "4f184c4a-df6b-46ef-8106-298105f70f20",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "SELECT * FROM delta.`dbfs:/databricks-datasets/nyctaxi-with-zipcodes/subsampled`",
      "label": "Validation Input",
      "name": "validation_input",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "SELECT * FROM delta.`dbfs:/databricks-datasets/nyctaxi-with-zipcodes/subsampled`",
      "label": "Validation Input",
      "name": "validation_input",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "validation_thresholds_loader_function": {
     "currentValue": "validation_thresholds",
     "nuid": "d34d43f5-a41b-4277-aa62-09c4b974c9e8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "validation_thresholds",
      "label": "Validation Thresholds Loader Function",
      "name": "validation_thresholds_loader_function",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "validation_thresholds",
      "label": "Validation Thresholds Loader Function",
      "name": "validation_thresholds_loader_function",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
